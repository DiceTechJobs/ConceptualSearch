# Config file for step 3 - train word2vec model

[DEFAULT]
# Folder containing the files output from the pre_process_documents.py script
# if you've skipped step 1 (pre_process_documents.py), the files in this folder should have any punctuation you don't want removed
# and each file should have each sentence on a new line
processed_documents_folder:			/Users/simon.hughes/Documents/Dice Data/LuceneTalk/ProcessedDocs

# Regex for matching files in documents folder above. Note this is a regex and not a file glob pattern
file_mask:							.*.txt

# A comma-delimited list of files used to parse keywords and phrases important to your domain
# This is important for extracting out multi word phrases from your documents
# Use a keywords file, and optionally the phrases auto-extracted using the #2 notebook or "extract_keywords.py" script
# You can also simply use a shingle filter within Solr to extract common phrases - set a document frequency limit, and then
# read the index values (use a plugin, or do a facet query on the shingled field, using a q=*.* to get everything)
# IMPORTANT - ensure booleans, parens and other logical operators removed from keywords (or exclude boolean searches altogether)
#
# format: specify one term or phrase per line in the file (solr stop words format)
keyword_files:                      /Users/simon.hughes/Documents/Dice Data/LuceneTalk/Phrases.txt,/Users/simon.hughes/Documents/Dice Data/LuceneTalk/top_5k_keywords.txt

# optional (can leave blank) - words to remove from consideration for training the model
# format: specify one term or phrase per line in the file (solr stop words format)
stop_words_file:                    /Users/simon.hughes/Software/Solr/solr-5.1.0/server/solr/DiceJobs/conf/dice_stop_words.txt

# Ignore sentences with few words than this (recommended - short sentences are noisy)
min_sentence_length_words:          5

# if not case_sensitive, everying will be lower case
case_sensitive:                     False

#Word 2 Vec algorithm settings - see https://radimrehurek.com/gensim/models/word2vec.html

[WORD2VEC]
#OUTPUT - serialization of the model to disk
word2vec_model_file:                /Users/simon.hughes/Documents/Dice Data/LuceneTalk/keyword_model.w2v

# Minimum word count (words below excluded)
min_word_count:                     5

# Number of elements in learned vectors. Normally 100-300, but depends on domain.
vector_size:                        300

# Number of concurrent processes
workers:                            8

# Number of iterations. Start with a low number (5-10) to train the initial model. Then try longer training times to see if accuracy improves
training_iterations:                10

# Size of the word window to used to train the model (predicts words within this window of the current word).
# The smaller the window, the more syntactically similar the words get (e.g. nouns become more similar to other nouns, etc).
# The wider the window, the more the focus is on semantic relatedness. 5-7 is a good range.
window_size:                        5